{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Some functions citied from w.deng\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from tqdm import  tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.special import expit\n",
    "import sklearn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (375806, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train = pd.read_csv(\"./input/train.csv\")\n",
    "test = pd.read_csv(\"./input/test.csv\")\n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 50000#max number of features\n",
    "max_len = 70 #max number of words.\n",
    "emb_path = './input/GoogleNews-vectors-negative300.bin'#path to the word2vec package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, lower=True)\n",
    "questions = list(train['question_text'].values)\n",
    "tokenizer.fit_on_texts(questions)\n",
    "\n",
    "train_tokenized = tokenizer.texts_to_sequences(train['question_text'].fillna('missing'))# to numbers\n",
    "test_tokenized = tokenizer.texts_to_sequences(test['question_text'].fillna('missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "test_X = pad_sequences(test_tokenized, maxlen=max_len)#fixed length\n",
    "train_y = train.target.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = KeyedVectors.load_word2vec_format(emb_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.mean(embeddings.vectors)\n",
    "sdv = np.std(embeddings.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, size = embeddings.vectors.shape\n",
    "num_words = min(len(tokenizer.word_index), max_features)\n",
    "embedding_matrix = np.random.normal(u, sdv, (num_words, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n"
     ]
    }
   ],
   "source": [
    "for word, index in tokenizer.word_index.items():#embedding matrix\n",
    "    if index < max_features:\n",
    "        try:\n",
    "            v = embeddings.get_vector(word)\n",
    "            embedding_matrix[index] = v\n",
    "        except:\n",
    "            pass\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use search function to search best threshld, considering multiple indicators\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "        y_pred = y_proba > threshold\n",
    "        score = f1_score(y_true, y_pred)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score, \n",
    "                     'roc-auc': sklearn.metrics.roc_auc_score(y_true, y_pred),\n",
    "                     'pr-auc': sklearn.metrics.average_precision_score(y_true, y_pred),\n",
    "                     'loss': sklearn.metrics.log_loss(y_true, y_pred)}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(max_features, size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=(3, size))\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=(4, size))\n",
    "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=(5, size))\n",
    "        \n",
    "        self.lstm = nn.LSTM(100, 50)\n",
    "        self.fc = nn.Linear(50, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        #self.gru = nn.GRU(100, 50, dropout=0.1)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        #print(\"embedding\",embedded.shape)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #print(\"embedding unsqueeze\",embedded.shape)\n",
    "        pooled_0 = F.adaptive_avg_pool1d(F.relu(self.conv_0(embedded).squeeze(3)), 50).squeeze(2)# convolution and pooling layers\n",
    "        pooled_1 = F.adaptive_avg_pool1d(F.relu(self.conv_1(embedded).squeeze(3)), 50).squeeze(2)\n",
    "        pooled_2 = F.adaptive_avg_pool1d(F.relu(self.conv_2(embedded).squeeze(3)), 50).squeeze(2)\n",
    "        #print('pooled_0', pooled_0.shape)\n",
    "\n",
    "        cat = torch.cat((pooled_0, pooled_1, pooled_2), dim=2)# concenate results together\n",
    "        #print('cat',cat.shape)\n",
    "        cat = self.dropout(cat)\n",
    "        #print('cat', cat.shape)\n",
    "        cat = cat.transpose(1,2).transpose(0,1)\n",
    "        #p=cat\n",
    "        #p = F.tanh(p)\n",
    "        # output, hidden = self.gru(p)\n",
    "        #print('cat', cat.shape)\n",
    "        output, (hn, cn) = self.lstm(cat)\n",
    "        k = self.fc(hn.squeeze(0))\n",
    "        return k\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1306122, 70)\n",
      "<class 'numpy.ndarray'> (1306122,)\n",
      "Fold:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baoji\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\baoji\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\baoji\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\baoji\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 \t loss=0.1246 \t val_loss=0.1155 \t time=338.20s\n",
      "Epoch 2/5 \t loss=0.1096 \t val_loss=0.1103 \t time=348.83s\n",
      "Epoch 3/5 \t loss=0.1024 \t val_loss=0.1096 \t time=338.14s\n",
      "Epoch 4/5 \t loss=0.0960 \t val_loss=0.1101 \t time=337.06s\n",
      "Epoch 5/5 \t loss=0.0900 \t val_loss=0.1127 \t time=344.93s\n",
      "Fold:1\n",
      "Epoch 1/5 \t loss=0.1247 \t val_loss=0.1167 \t time=338.74s\n",
      "Epoch 2/5 \t loss=0.1102 \t val_loss=0.1112 \t time=342.06s\n",
      "Epoch 3/5 \t loss=0.1020 \t val_loss=0.1100 \t time=347.88s\n",
      "Epoch 4/5 \t loss=0.0943 \t val_loss=0.1126 \t time=344.39s\n",
      "Epoch 5/5 \t loss=0.0869 \t val_loss=0.1128 \t time=336.52s\n",
      "Fold:2\n",
      "Epoch 1/5 \t loss=0.1245 \t val_loss=0.1140 \t time=339.26s\n",
      "Epoch 2/5 \t loss=0.1098 \t val_loss=0.1105 \t time=338.23s\n",
      "Epoch 3/5 \t loss=0.1028 \t val_loss=0.1086 \t time=336.87s\n",
      "Epoch 4/5 \t loss=0.0963 \t val_loss=0.1102 \t time=340.76s\n",
      "Epoch 5/5 \t loss=0.0903 \t val_loss=0.1124 \t time=337.66s\n",
      "Fold:3\n",
      "Epoch 1/5 \t loss=0.1255 \t val_loss=0.1143 \t time=336.36s\n",
      "Epoch 2/5 \t loss=0.1112 \t val_loss=0.1112 \t time=340.31s\n",
      "Epoch 3/5 \t loss=0.1043 \t val_loss=0.1091 \t time=338.68s\n",
      "Epoch 4/5 \t loss=0.0980 \t val_loss=0.1093 \t time=334.29s\n",
      "Epoch 5/5 \t loss=0.0922 \t val_loss=0.1111 \t time=334.56s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=0).split(train_X, train_y))\n",
    "train_epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "print(type(train_X), train_X.shape)\n",
    "print(type(train_y), train_y.shape)\n",
    "train_X = torch.Tensor(train_X)\n",
    "train_y = torch.Tensor(train_y)\n",
    "\n",
    "train_preds = np.zeros((len(train_X)))\n",
    "test_preds = np.zeros((len(test_X)))\n",
    "\n",
    "x_test_cuda = torch.tensor(test_X, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):# cross validation\n",
    "    print('Fold:{0}'.format(i))\n",
    "    x_train_fold = torch.tensor(train_X[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    x_val_fold = torch.tensor(train_X[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    train_data = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid_data = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = CNN()\n",
    "    model.cuda()\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    optimiser = optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(train_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.0\n",
    "        # train  model\n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            y_predicted = model(x_batch)\n",
    "            loss = loss_function(y_predicted, y_batch)\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            avg_loss += loss.item()/len(train_loader)\n",
    "\n",
    "        # evaluate model\n",
    "        model.eval()\n",
    "        val_predicted_fold = np.zeros(x_val_fold.size(0))\n",
    "        test_predicted_fold = np.zeros(len(test_X))\n",
    "        avg_val_loss =0.0\n",
    "\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_predicted = model(x_batch).detach()\n",
    "            avg_val_loss += loss_function(y_predicted, y_batch).item()/len(valid_loader)\n",
    "            val_predicted_fold[i*batch_size:(i+1)*batch_size] = expit(y_predicted.cpu().numpy())[:,0]   \n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(epoch + 1,\n",
    "                                                        train_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "    # predict testing dataset\n",
    "    test_predicted_fold = np.zeros(len(test_X))\n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        test_predicted_fold[i * batch_size:(i+1) * batch_size] = expit(y_pred.cpu().numpy())[:, 0]\n",
    "    \n",
    "    train_preds[valid_idx] = val_predicted_fold\n",
    "    test_preds += test_predicted_fold / len(splits) # take average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████████████████████████████████████████████▏| 99/100 [00:31<00:00,  3.46it/s]C:\\Users\\baoji\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:32<00:00,  3.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.32,\n",
       " 'f1': 0.6418220905235024,\n",
       " 'roc-auc': 0.5,\n",
       " 'pr-auc': 0.06187017751787352,\n",
       " 'loss': 2.1369202268032685}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result = threshold_search(train_y, train_preds)\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
